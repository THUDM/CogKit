# ================ Logging ================
name4train: "t2i-train"
log_level: "INFO"  # Options: ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]

# ================ Model ================
model_name: "cogview4-6b"  # Options: ["cogview4-6b"]
model_path: "THUDM/CogView4-6B"


# ================ Output ================
output_dir: "/path/to/output"


# ================ Tracker ================
report_to: null  # Options: ["wandb"]


# ================ Data ================
data_root: "/path/to/t2i/data"

# ================ Training ================
seed: 42
training_type: "lora"   # Options: ["lora", "sft"]

strategy: "DDP"  # Options: ["DDP", "SHARD_GRAD_OP", "FULL_SHARD", "HYBRID_SHARD", "_HYBRID_SHARD_ZERO2"]

# This will offload model param and grads to CPU memory to save GPU memory, but will slow down training
offload_params_grads: false

# This will increase memory usage since gradients are sharded during accumulation step.
# Note: When used with offload_params_grads, model parameters and gradients will only be offloaded
#   to the CPU during the final synchronization (still retained on GPU in gradient accumulation steps)
#   which means offload_params_grads is meaningless when used with no_grad_sync_when_accumulating
no_grad_sync_when_accumulating: false

# When enable_packing is true, training will use the native image resolution,
#   otherwise all images will be resized to train_resolution, which may distort the original aspect ratio.
# IMPORTANT: When changing enable_packing from true to false (or false to true),
#   make sure to clear the `.cache` directories in your `data_root/train` and `data_root/test` folders if they exist.
enable_packing: false

# This will slow down validation speed and enable quantization during training to save GPU memory
low_vram: false

# Note: For CogView4 series models, height and width should be **32N** (multiple of 32)
train_resolution: [1024, 1024]  # [Height, Width]

train_epochs: 1
batch_size: 1
gradient_accumulation_steps: 1
mixed_precision: "bf16"  # Options: ["fp32", "fp16", "bf16"]
learning_rate: 2.0e-5

num_workers: 8
pin_memory: true

checkpointing_steps: 10
checkpointing_limit: 2
resume_from_checkpoint: null  # or "/path/to/checkpoint/dir"


# ================ Validation ================
do_validation: true
validation_steps: 10  # Must be a multiple of `checkpointing_steps`
